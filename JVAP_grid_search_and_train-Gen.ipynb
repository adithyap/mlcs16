{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,make_scorer,f1_score,classification_report,average_precision_score\n",
    "from sklearn.preprocessing import Normalizer,MinMaxScaler,StandardScaler,normalize\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_columns',1000)\n",
    "pd.set_option('max_rows',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# PANDAS HELPERS\n",
    "#############################################\n",
    "\n",
    "def remove_column_from_data_frame(col_to_remove, data_frame):\n",
    "\n",
    "    if col_to_remove in list(data_frame.columns):\n",
    "        data_frame.drop(col_to_remove, axis=1, inplace=True)\n",
    "\n",
    "        \n",
    "def remove_columns_from_data_frame(cols_to_remove, data_frame):\n",
    "\n",
    "    column_dict = {x: None for x in list(data_frame.columns)}\n",
    "\n",
    "    cols_to_remove = [x for x in cols_to_remove if x in column_dict]\n",
    "\n",
    "    data_frame.drop(labels=cols_to_remove, axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "def remove_columns_like(column_pattern, data_frame):\n",
    "    \n",
    "    for column in list(data_frame.columns):\n",
    "        if column_pattern in column:\n",
    "            data_frame.drop(column, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "def fill_nas(value, data_frame):\n",
    "    \n",
    "    data_frame.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# DATA RETRIEVAL HELPERS\n",
    "#############################################\n",
    "\n",
    "def get_data(n_rows=None):\n",
    "\n",
    "    if n_rows is not None:\n",
    "        df = pd.read_csv('final_feats_without_dummies_3.csv', low_memory=False, nrows=n_rows)\n",
    "        df_y = pd.read_csv('final_outs_3.csv', low_memory=False, nrows=n_rows)\n",
    "    else:\n",
    "        df = pd.read_csv('final_feats_without_dummies_3.csv', low_memory=False)\n",
    "        df_y = pd.read_csv('final_outs_3.csv', low_memory=False)\n",
    "    \n",
    "    \n",
    "    # Drop labels and a redundant column\n",
    "    remove_columns_from_data_frame(['Unnamed: 0', 'Unnamed: 0.1' 'dissent', 'dissentdummy'], df)\n",
    "    df,df_y=remove_bad_rows(df,df_y)\n",
    "    df=drop_unneeded_cols(df)\n",
    "    df_x=dummify(df_x)\n",
    "    \n",
    "    # Extras -- for analysis\n",
    "    # CASE 1: REMOVE TOP 2\n",
    "    # CASE 2: REMOVE ALL 'DISS'\n",
    "    \n",
    "#     remove_columns_from_data_frame(['type', 'turnonthresh'], df)\n",
    "#     remove_columns_from_data_frame(['type1', 'last3'], df)\n",
    "#     remove_columns_like('diss', df)\n",
    "    if ('Unnamed: 0' or 'Unnamed: 0.1') in df_y.columns:\n",
    "        df_y.drop(labels=['Unnamed: 0','Unnamed: 0.1'],axis=1,inplace=True)\n",
    "    \n",
    "    return df, df_y\n",
    "\n",
    "\n",
    "def get_x_y(n_rows=None):\n",
    "    \n",
    "    df, df_y = get_data(n_rows)\n",
    "\n",
    "    #fill_nas(0, df)\n",
    "    \n",
    "    return df.values, df_y.ix[:,0].values\n",
    "\n",
    "\n",
    "def get_columns(df):\n",
    "    \n",
    "    #df = pd.get_dummies(pd.read_csv('final_feats_without_dummies.csv', low_memory=False, nrows=2))\n",
    "    return list(df.columns)\n",
    "\n",
    "\n",
    "def print_report(y, y_pred):\n",
    "\n",
    "    print classification_report(y, y_pred)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# MODEL HELPERS\n",
    "#############################################\n",
    "\n",
    "def grid_search(X, y, clf, param_grid):\n",
    "    \n",
    "#     param_dict={'average': 'weighted'}\n",
    "    scorer = make_scorer(average_precision_score)\n",
    "\n",
    "\n",
    "    gridclf = GridSearchCV(clf, paramgrid, scoring=scorer, cv=3, verbose=1)\n",
    "\n",
    "    gridclf.fit(X, y)\n",
    "\n",
    "    print gridclf.best_params_\n",
    "    print gridclf.best_estimator_\n",
    "\n",
    "    print_report(y_test, gridclf.predict(X_test))\n",
    "    \n",
    "\n",
    "def get_top_n(n, arr, col_names, prev_list=[]):\n",
    "    \n",
    "    if n <= 0:\n",
    "        return []\n",
    "    \n",
    "    most_imp = -1\n",
    "    most_imp_index = -1\n",
    "\n",
    "    for i in range(len(arr)):\n",
    "\n",
    "        if i in prev_list:\n",
    "            continue\n",
    "\n",
    "        if arr[i] > most_imp:\n",
    "            most_imp = arr[i]\n",
    "            most_imp_index = i\n",
    "\n",
    "    prev_list.append(most_imp_index)\n",
    "\n",
    "    return [ (col_names[most_imp_index], most_imp) ] + get_top_n(n - 1, arr, col_names, prev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_unneeded_cols(df):\n",
    "    del_cols = ['fileid','cite','vol','beginpg','endopin','endpage','docnum','priorpub','_merge','year',\n",
    "            'circuit','pseatno','decision_date','aatty_first_name','aatty_last_name','afirm_name',\n",
    "            'ratty_first_name','ratty_last_name','rname_of_first_listed_amicus_gro','rfirm_namew','decisiondatenew2',\n",
    "           'j1name','j2name','j3name','quartertoelect','pname','seatno','success','lsuc','ls1','ls2','ls3','lp',\n",
    "            'lp2','lp3','sseatno','congress','congreso','afirst_listed_amicus_group','yearquarter','name','Name','State','j',\n",
    "            'codej4','j4vote1','j4vote2','j4maj1','j4maj2','codej5','j5vote1','j5vote2','j5maj1','j5maj2',\n",
    "            'codej6','j6vote1','j6vote2','j6maj1','j6maj2','codej7','j7vote1','j7vote2','j7maj1','j7maj2',\n",
    "            'codej8','j8vote1','j8vote2','j8maj1','j8maj2','codej9','j9vote1','j9vote2','j9maj1','j9maj2',\n",
    "            'codej10','j10vote1','j10vote2','j10maj1','j10maj2','codej11','j11vote1','j11vote2','j11maj1','j11maj2',\n",
    "            'codej12','j12vote1','j12vote2','j12maj1','j12maj2','codej13','j13vote1','j13vote2','j13maj1','j13maj2',\n",
    "            'codej14','j14vote1','j14vote2','j14maj1','j14maj2','codej15','j15vote1','j15vote2','j15maj1','j15maj2','j16maj1','j16vote1']\n",
    "    df.drop(labels=del_cols,axis=1,inplace=True)\n",
    "    moredropcolumns=df.columns.tolist() # .tolist?\n",
    "    for i in moredropcolumns:\n",
    "        if len(pd.unique(df[i]))==1:\n",
    "            df.drop(labels=i,axis=1,inplace=True)\n",
    "    df.drop(labels=['casenum','j2vote1','j2vote2','j2maj1','direct1',\n",
    "                          'j2maj2','j3vote1','j3vote2','j3maj1','j3maj2','majvotes','ids'],axis=1,inplace=True)\n",
    "    return df\n",
    "    \n",
    "def dummify(df):\n",
    "    new_cols=df.columns\n",
    "    new_cols=new_cols.tolist()\n",
    "#     keep_cols=['j1score','j2score','j3score','popularpct','electoralpct','closerd','fartherd','dAds3','dF2Ads3',\n",
    "#            'dF1Ads3','dL1Ads3','dL2Ads3','dL3Ads3','dL4Ads3','dL5Ads3','logAds3','logL1Ads3','logL2Ads3','logF1Ads3',\n",
    "#           'logF2Ads3','decade2','propneg','likely_elev2','score','d12','d13','d23','sat_together_count']\n",
    "\n",
    "    float_cols=['j1score','j2score','j3score','popularpct','electoralpct','closerd','fartherd','dAds3','dF2Ads3',\n",
    "           'dF1Ads3','dL1Ads3','dL2Ads3','dL3Ads3','dL4Ads3','dL5Ads3','logAds3','logL1Ads3','logL2Ads3','logF1Ads3',\n",
    "          'logF2Ads3','decade2','propneg','likely_elev2','score','d12','d13','d23',\n",
    "           'judgecitations','experience','experiencetrun','age2trun','agego','assets','ba','liable',\n",
    "            'networth','totalcities','sat_together_count','keytotal','lengthopin','Wopinionlenght','Wtotalcites','age']\n",
    "\n",
    "    remove_for_now=['Ads3','F1Ads3','F2Ads3','L1Ads3','L2Ads3','L3Ads3','L4Ads3','L5Ads3','Unnamed: 0.1','appel1','appel2',\n",
    "               'citevol','codej3','id','usc2sect','usc1sect','age2','distjudg','respond1','respond2','yearb','pred','csb']\n",
    "\n",
    "    df_x.drop(labels=remove_for_now,inplace=True,axis=1)\n",
    "    sum1=0\n",
    "    \n",
    "    dummy_cols=[]\n",
    "    for col in df.columns:\n",
    "        if col not in float_cols:\n",
    "            if len(pd.unique(df.ix[:,col]))>100 or (df.ix[:,col].dtype!='float64' and df.ix[:,col].dtype!='int64'): \n",
    "                sum1+= len(pd.unique(df.ix[:,col]))\n",
    "                dummy_cols.append(col)\n",
    "    print \"# of dummy columns: \",sum1\n",
    "    df2=pd.get_dummies(df,columns=dummy_cols,dummy_na=True,sparse=True)\n",
    "    df2=df2.fillna(value=0,inplace=True)\n",
    "    return df2\n",
    "\n",
    "\n",
    "def remove_bad_rows(df_x,df_y):\n",
    "    \n",
    "    #remove rows where codej1==codej2\n",
    "#     df[df.codej1==df.codej2].index\n",
    "    same_cols = df_x[df_x.codej1==df_x.codej2].index\n",
    "    df_x=df_x.drop(same_cols).reset_index(drop=True)\n",
    "    df_y=df_y.drop(same_cols).reset_index(drop=True)\n",
    "    #remove rows where >3 judges occur\n",
    "#     pp = pd.read_csv('../raw/Votelevel_stuffjan2013.csv')\n",
    "#     qq=pp.groupby(by=['casenum']).count()\n",
    "#     pd.unique(qq.month)\n",
    "#     rr=qq[qq.month==6].reset_index()\n",
    "#     rr.shape\n",
    "    \n",
    "    #remove rows where codej2==null\n",
    "    #df[map(lambda x: not(x),pd.notnull(df.ix[:][\"codej2\"]).tolist())]\n",
    "    nan_cols=df_x[map(lambda x: not(x),pd.notnull(df_x.ix[:][\"codej2\"]).tolist())].index\n",
    "    nan_cols.append(df_x[map(lambda x: not(x),pd.notnull(df_x.ix[:][\"codej1\"]).tolist())].index)\n",
    "    df_x=df_x.drop(nan_cols).reset_index(drop=True)\n",
    "    df_y=df_y.drop(nan_cols).reset_index(drop=True)\n",
    "    \n",
    "    return df_x,df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Read data into X and y\n",
    "#############################################\n",
    "\n",
    "X, y = get_x_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111538, 5032)\n",
      "(111538,)\n"
     ]
    }
   ],
   "source": [
    "print X.shape\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.  11.  11. ...,   0.   0.   0.]\n",
      " [  1.  11.  11. ...,   0.   0.   0.]\n",
      " [  1.  11.  11. ...,   0.   0.   0.]\n",
      " ..., \n",
      " [  1.   5.   5. ...,   0.   0.   0.]\n",
      " [  1.   5.   5. ...,   0.   0.   0.]\n",
      " [  1.   5.   5. ...,   0.   0.   0.]]\n",
      "[1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "#sanity check\n",
    "print X[:10]\n",
    "print y[:10]\n",
    "\n",
    "#check that y has all 1's and -1's. Sometimes y can erronously have indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Split into training and test set\n",
    "#############################################\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print X.nbytes/1024/1024/1024 #size of X in GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 111538 entries, 0 to 111537\n",
      "Columns: 5032 entries, ElecYear_AndPrior to totalcites_nan\n",
      "dtypes: float64(5031), int64(1)\n",
      "memory usage: 607.6 MB\n"
     ]
    }
   ],
   "source": [
    "print df_x.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78076, 5032)\n",
      "(78076,)\n",
      "(33462, 5032)\n",
      "(33462,)\n"
     ]
    }
   ],
   "source": [
    "#check sizes match\n",
    "\n",
    "print X_train.shape\n",
    "print y_train.shape\n",
    "print X_test.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#DONT DO FOR RANDOM FOREST\n",
    "\n",
    "# #############################################\n",
    "# # Standard scale\n",
    "# #############################################\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train)\n",
    "\n",
    "# X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# [OPTIONAL]\n",
    "# Random Forest Grid Search\n",
    "#############################################\n",
    "\n",
    "paramgrid = {'n_estimators': [10, 50, 100], 'max_depth': [1, 5, 10, 15]}\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid_search(X_train, y_train, rf_clf, paramgrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.73      0.49      0.58      2280\n",
      "          1       0.98      0.99      0.99     53553\n",
      "\n",
      "avg / total       0.97      0.97      0.97     55833\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# Random Forest\n",
    "#############################################\n",
    "\n",
    "# Replace labels (in case SVM was run)\n",
    "# y_train[y_train == 0.] = -1.\n",
    "# y_test[y_test == 0.] = -1.\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=42, \n",
    "                                n_estimators=100, \n",
    "                                max_depth=15, \n",
    "#                                 class_weight={1.0: 1, -1.0: 150})\n",
    "                                )\n",
    "\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "print_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('type1', 0.085836245627072427)\n",
      "('last3', 0.057453894299753429)\n",
      "('close2', 0.013785569650024256)\n",
      "('close3', 0.013742677538063049)\n",
      "('diss0promerdummy', 0.0065234843229324416)\n",
      "('unanimous', 0.0057345487383386635)\n",
      "('dissent', 0.0047420628379996922)\n",
      "('din', 0.0043813667302849586)\n",
      "('concprodummy', 0.0036449179471683664)\n",
      "('keytotal', 0.0035560335123791609)\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# [OPTIONAL]\n",
    "# Feature importance analysis\n",
    "#############################################\n",
    "\n",
    "top_n = get_top_n(10, rf_clf.feature_importances_, get_columns())\n",
    "\n",
    "for t in top_n:\n",
    "    print t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    4.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed:   15.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n",
      "{'kernel': 'poly', 'max_iter': 1000, 'coef0': 1000.0, 'degree': 1, 'class_weight': {1.0: 1, -1.0: 150}}\n",
      "SVC(C=1.0, cache_size=200, class_weight={1.0: 1, -1.0: 150}, coef0=1000.0,\n",
      "  decision_function_shape=None, degree=1, gamma='auto', kernel='poly',\n",
      "  max_iter=1000, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.00      0.00      0.00        11\n",
      "          1       0.98      1.00      0.99       489\n",
      "\n",
      "avg / total       0.96      0.98      0.97       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed:   18.5s finished\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# [OPTIONAL]\n",
    "# SVM Grid Search\n",
    "#############################################\n",
    "\n",
    "paramgrid = {'kernel': ['rbf', 'poly', 'sigmoid', 'linear'], \n",
    "             'degree': [1, 3, 5, 7, 9], \n",
    "             'coef0': [1e-3, 1e-1, 1e1, 1e3], \n",
    "             'max_iter': [1000], \n",
    "             'class_weight': [{1.0: 1, -1.0: 150}]}\n",
    "\n",
    "svm_clf = SVC()\n",
    "\n",
    "grid_search(X_train, y_train, svm_clf, paramgrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.00      0.00      0.00        11\n",
      "          1       0.98      1.00      0.99       489\n",
      "\n",
      "avg / total       0.96      0.98      0.97       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SVM\n",
    "#############################################\n",
    "\n",
    "# Replace labels\n",
    "# y_train[y_train == 0.] = -1.\n",
    "# y_test[y_test == 0.] = -1.\n",
    "\n",
    "svm_clf = SVC(kernel='rbf', max_iter=1000, coef0=1e-3, degree=2, class_weight={1.0: 1, -1.0: 150})\n",
    "\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "print_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tdf = pd.get_dummies(pd.read_csv('final_outs.csv', low_memory=False))\n",
    "\n",
    "# pd.unique(tdf.ix[:,1].values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
